---
# title: "classification"
author: "wiesehahn"
date: "2020-07-28"
output: workflowr::wflow_html
editor_options:
  chunk_output_type: console
---

```{r global_options, include=FALSE}
knitr::opts_chunk$set(# fig.width=12, fig.height=8, 
  echo=FALSE, warning=FALSE, message=FALSE)
```

```{r setup, include=FALSE}
library(viridis)
library(kableExtra)
library(readr)
library(plotly)
library(here)
library(recipes)
library(caret)
library(ranger)
```


## Model Tuning

### Hyperparameter

A number of model parameters can be changed to achieve different results. Model tuning changes these parameters to get optimal results. Overfitting might be a problem. Parameters to be changed in all classifiers are input variables (bands and indices in our case) and training data. 

Hyperparameters in the random forest package include:

- ntree (Integer, default: 500):
Number of trees to grow.

- mtry (Integer, Defaults to the square root of the number of variables)
The number of variables per split. 

- nodesize (Integer, default: 1):
The minimum size of a terminal node.


#### Gridsearch Results

The *number of trees* was held constant at a value of 500 because generally it is assumed that more trees give better performance. On the other side larger numbers also mean more processing time.
Parameters for which a gridsearch was done are the *number of variables per split* and the *minimum terminal node size*. 

```{r read, message=FALSE, warning=FALSE}

filename = here("data/reference/train_test/train_test.rds")

if (file.exists(filename)){
  
  ref <- readRDS(filename)
  
} else{
  
# read reference data exported by google earth engine
train_test <- read_csv(here("data/reference/train_test/S2B_MSIL2A_20170823T103019_N0205_R108_T32UNC_20170823T103018_Dsen2_TopCorSlope10K06ndvi.csv"))

# as factor
cols <- c("BA", "Baumart", "wbz", "Gebiet")
train_test[cols] <- lapply(train_test[cols], as.factor)
levels(train_test$Baumart) <- c("BU", "DGL", "FI", "KI", "LAE", "TEI")

# rename
ref <- train_test %>% 
  rename_with(~ gsub("S2B_MSIL2A_20170823T103019_N0205_R108_T32UNC_20170823T103018_Dsen2_TopCorSlope10K06ndvi", "band", .x, fixed = TRUE))

# subset
ref <- ref %>% 
  select(-X1, -ID, -BA, - wbz, -Gebiet)

  
  # save data
  saveRDS(ref, filename)
  
}


# split in train and test data
index <- createDataPartition(y = ref$Baumart, p = .7, list = FALSE)
training <- ref[index, ]
testing <- ref[-index, ]
```


```{r gridsearch}
# create random forest model
filename = here("data/models/rf_all_fit.rds")
if (file.exists(filename)){
  
  rf_all_fit <- readRDS(filename)
  
} else{
  
  # specify that the resampling method is 
  fit_control <- trainControl(## 10-fold CV
    method = "cv",
    number = 10)
  
  # define a grid of parameter options to try
  rf_grid <- expand.grid(mtry = c(2, 3, 4, 6, 8),
                         splitrule = c("gini"),
                         min.node.size = c(1, 5, 10))
  
  # run a random forest model
  set.seed(825)
  library(doParallel)
  cl <- makePSOCKcluster(4)
  registerDoParallel(cl)
  
  # fit the model with the parameter grid
  rf_all_fit <- train(Baumart ~ ., 
                  data = training, 
                  method = "ranger",
                  importance = "impurity" ,
                  trControl = fit_control,
                  # provide a grid of parameters
                  tuneGrid = rf_grid)
  
  stopCluster(cl)
  
  # save model
  saveRDS(rf_all_fit, filename)
  
}
```


```{r gridsearch_result, fig.cap = "R output of gridsearch result for number of variables per split and minimal node size"}
print(rf_all_fit)
```


```{r plot_gridsearch_result, fig.cap= "plotted gridsearch result for the number of predictor variables and minimal node size"}

plot_ly(rf_all_fit$results, y = ~Accuracy, x = ~as.factor(mtry), color = ~as.factor(min.node.size), type = "box") %>%
    layout(xaxis = list(title="Randomly Selected Predictors at each split"),
         yaxis = list(title="Accuracy (Cross-Validation)")) 

```


As we can see the differences are not very pronounced. Hence, a simpler model results in similar performance. Based on these results we can simplify our model in terms of minimal node size and variables per split without deteriorating the results.

-----------------

#### Model Simplification

Here we search for the simplest model without deteriorating the performance (max 2% difference to best model).

```{r gridsearch_result_simp, fig.cap= "R output for simplified model"}
# get simplest model with similar accuracy
whichTwoPct <- tolerance(rf_all_fit$results, metric = "Accuracy", 
                         tol = 2, maximize = TRUE)  

kable(rf_all_fit$results[whichTwoPct,], row.names = F, align="c") %>% kable_styling(full_width = F)
```

The results indicate that in terms of prediction accuracy a model with just `r rf_all_fit$results[whichTwoPct,"mtry"]` variables per split and a minimal node size of `r rf_all_fit$results[whichTwoPct,"min.node.size"]` is sufficient enough.

-----------------


#### Model Validation

To have a closer look at the model performance the validation data is classified with the best model obtained by gridsearch.


**Error Matrix**
```{r gridsearch_validation_matrix, fig.cap = "Error matrix for gridseach optimzed and simplified rf-model"}
# validation

model.all <- ranger(formula = Baumart ~ ., 
                        data = training,
                        num.trees = 500, 
                        mtry = 2,
                        min.node.size = 1,
                        importance = "impurity")

testing.pred.all <- predict(model.all, testing)

cm.all<- confusionMatrix(data = testing.pred.all$predictions, reference = testing$Baumart)

cm.all.df <- as.data.frame.matrix(cm.all$table)


# testing.pred.all <- predict(rf_all_fit, newdata = testing %>% select( -Baumart), probability= F)
# cm.all<- confusionMatrix(data = testing.pred.all, reference = testing$Baumart)
# 
# cm.all.df <- as.data.frame.matrix(cm.all$table)

cm.all.df %>%
  mutate_all(~ifelse(. > 400,
                  cell_spec(., "html", color = "black", bold = T),
                  ifelse(. > 0,
                         cell_spec(., color = "white", bold = T, background = spec_color(., option = "A", direction= -1, scale_from = c(0,nrow(testing)/50))),
                         cell_spec(., "html", color = "grey")
                         )
                  )
             ) %>%
  mutate(Baumart = cell_spec(c("BU","DGL","FI","KI", "LAE","TEI"), "html",bold = T)) %>%
  select(Baumart, everything(.)) %>%
  
  kable("html", escape = F, rownames = TRUE, align=c('r', rep('c', 6))) %>%
  add_header_above(c("", "Reference" = 6)) %>%
  kable_styling("hover", full_width = F)
```



**Respective Accuracy**
```{r gridsearch_validation_accuracy, message=FALSE, warning=FALSE}
cm.all.acc.df <- as.data.frame(t(cm.all$overall[1:4]))

kable(cm.all.acc.df) %>% kable_styling(full_width = F)
```


-----------------

### Variable Importance

As determined before, model parameters *minimal node size* and *variables per split* have limited influence on model performance. As a consequence it is likely that the choice of predictor variables is important for model performance.  

Input variables considered as predictor variables in this study comprise: 

*Sentinel-2 bands and indices*
- Bands: *B2*,*B3*,*B4*,*B5*,*B6*,*B7*,*B8*,*B8A*,*B11*,*B12* ???
- Indices: ???

In model fitting the relative variable importance is calculated to give an impression which predictor variables are valuable and which are less valuable for the prediction process. However, correlation between variables is not taken into account.


```{r plot_gridsearch_importance, fig.cap= "Relative predictor variable importance"}
# variable importance
imp <- varImp(rf_all_fit)

plot_ly(imp$importance, y = ~reorder(row.names(imp$importance),Overall), x = ~Overall, type = 'bar', orientation = 'h') %>%
  layout(xaxis = list(title="Importance"),
         yaxis = list(title=""))

```

As we can see the importance metric varies between predictor variables, suggesting that the choice of predictor variables very much influences our model. While more predictor variables might add information they are also complicating the model and might even introduce noise. Hence, a reduction of predictor variables might enhance our model.

-----------------


#### Feature Selection

To further simplify the prediction model a *Recursive Feature Elimitaion* (rfe) is applied. This will eliminate worst performing predictor variables (chosen by importance) at each step and keep the best performing variables to end up in a reduced number of predictor variables which perform best in model prediction.

```{r rfe_init, message=FALSE, warning=FALSE}
# perform reverse feature selection with all variables
filename = here("data/models/rfProfile_all.rds")
if (file.exists(filename)){
  
  rfProfile <- readRDS(filename)
  
  } else{
    # normalize data
    training_recipe <- recipe(Baumart ~ ., data = training) %>%
      step_center(all_predictors()) %>%
      step_scale(all_predictors()) %>%
      step_nzv(all_predictors()) 
    
    train_prepped <- 
      training_recipe %>% 
      prep(training) %>% 
      juice()
    
    # number of features to test
    subsets <- c(1:26)
    
    training_ctrl <- rfeControl(
      method = "repeatedcv",
      repeats = 5,
      functions = rfFuncs, 
      returnResamp = "all"
    )
    
    library(doParallel)
    cl <- makePSOCKcluster(4)
    registerDoParallel(cl)
    
    rfProfile <- rfe(x = train_prepped %>% dplyr::select(-Baumart),
                     y = train_prepped$Baumart,
                     sizes = subsets,
                     metric = "Accuracy",
                     rfeControl = training_ctrl)
    
    stopCluster(cl)
    
    # save model
    saveRDS(rfProfile, filename)
    
    }
```

-----------------

#### Number of Features

```{r rfe_plot_model_performance, echo=FALSE, fig.cap= "model performance by number of features evaluated with Recursive Feature Elimitaion"}

plot_ly(rfProfile$resample, y = ~Accuracy, x = ~as.factor(Variables),  type = "box") %>%
  layout(xaxis = list(title="Number of predictor variables"),
         yaxis = list(title="Accuracy"))
 
```

The best model in regards to predictor variables uses `r rfProfile$results[which.max(rfProfile$results$Accuracy),]$Variables` out of 26 variables. However, we can see that the model performs equally good with less predictor variables.

**Chosen variables**

```{r rfe_tab_predictors, echo=FALSE}
predictors.df <- as.data.frame(predictors(rfProfile)) %>% rename("prediction features" = "predictors(rfProfile)")

kable(predictors.df, row.names = T) %>% kable_styling(full_width = F, position = "left")
```

**Respective Accuracy**

```{r rfe_model_accuracy, echo=FALSE, fig.cap= "model performance"}

kable(rfProfile$results[which.max(rfProfile$results$Accuracy),], row.names = F) %>% kable_styling(full_width = F)

```

-----------------

#### Model Simplification

To simplify the model without loosing prediction accuracy we search for a model with less predictor variables, which has the same accuracy as the best performing model (max 2 % difference in accuracy).

```{r rfe_simp_model, echo=FALSE, fig.cap= "RFE-simplified model performance"}
# get simplest model with similar accuracy
whichTwoPct_variables <- tolerance(rfProfile$results, metric = "Accuracy", 
                         tol = 2, maximize = TRUE)

var_num <- rfProfile$results[whichTwoPct_variables,"Variables"]
```

As a result we get a model using the following `r var_num` prediction variables instead of all 26 variables, which has almost the same accuracy.

**Chosen variables**

```{r rfe_tab_predictors_simp}

selectedVars <- rfProfile$variables
bestVar <- rfProfile$control$functions$selectVar(selectedVars, var_num)

bestVar.df <- as.data.frame(bestVar) %>% rename("prediction features" = bestVar)

kable(bestVar.df, row.names = T) %>% kable_styling(full_width = F, position = "left")
```

**Respective Accuracy**

```{r rfe_simp_model_accuracy, echo=FALSE, fig.cap= "RFE-simplified model performance"}

kable(rfProfile$results[whichTwoPct_variables,], row.names = F) %>% kable_styling(full_width = F)

```

-----------------

### Final Model

Using the results from previous analysis we train a model with best performing predictor variables and model-hyperparameters. 

The *predictor variables* are:

```{r rfe_tab_predictors_simp2}
kable(bestVar.df, row.names = T) %>% kable_styling(full_width = F, position = "left")
```


The *hyperparameters* are:

* Number of variables to possibly split at in each node (mtry) = `r rf_all_fit$results[whichTwoPct,"mtry"]`
* Minimal node size = `r rf_all_fit$results[whichTwoPct,"min.node.size"]`
* Number of trees = 500 (this was not optimized, as more trees usually give better results but the maximum number is limited by computation power)

-----------------

#### Model Validation

Applying the final model to predict tree species for the validation data set, the error matrix looks like this: 

<br>

**Error Matrix**
```{r rfe_validation_matrix, message=FALSE, warning=FALSE, fig.cap = "Error matrix for RFE- and gridsearch-optimzed rf-model"}

f <- as.formula(paste("Baumart", paste(bestVar, collapse=" + "), sep=" ~ "))

model.simp <- ranger(formula = f, 
                        data = training,
                        num.trees = 500, 
                        mtry = rf_all_fit$results[whichTwoPct,"mtry"],
                        min.node.size = rf_all_fit$results[whichTwoPct,"min.node.size"],
                        importance = "impurity")

testing.pred.simp <- predict(model.simp, testing)

cm.simp<- confusionMatrix(data = testing.pred.simp$predictions, reference = testing$Baumart)

cm.simp.df <- as.data.frame.matrix(cm.simp$table)

cm.simp.df %>%
  mutate_all(~ifelse(. > 200,
                  cell_spec(., "html", color = "black", bold = T),
                  ifelse(. > 0,
                         cell_spec(., color = "white", bold = T, background = spec_color(., option = "A", direction= -1, scale_from = c(0,nrow(testing)/50))),
                         cell_spec(., "html", color = "grey")
                         )
                  )
             ) %>%
  
 mutate(Baumart = cell_spec(c("BU","DGL","FI","KI", "LAE","TEI"), "html",bold = T)) %>%
  select(Baumart, everything(.)) %>%
  kable("html", escape = F, rownames = TRUE, align=c('r', rep('c', 6))) %>%
  add_header_above(c("", "Reference" = 6)) %>%
  kable_styling("hover", full_width = F)

```

<br>

**Respective Accuracy**

```{r rfe_validation_accuracy, message=FALSE, warning=FALSE}
cm.simp.acc.df <- as.data.frame(t(cm.simp$overall[1:4]))

kable(cm.simp.acc.df) %>% kable_styling(full_width = F)
```




-----------------

### Comparison to other models 

(models use the same hyperparameters as above but different predictor variable sets)

```{r rf_init}
# for reproducibility
set.seed(42)

# random forest parameters
ntree <- 500
nodesize <- rf_all_fit$results[whichTwoPct,"min.node.size"]
splitvariables <- rf_all_fit$results[whichTwoPct,"mtry"]

```

#### Model using Sentinel-2 bands

```{r rf_bands}
# predict on sentinel-2 bands only

predictors <- names(training %>% select(- contains("_VI"), -Baumart))

f <- as.formula(paste("Baumart", paste(predictors, collapse=" + "), sep=" ~ "))

# ranger RF model
rf_bands <- ranger(
    formula   = f, 
    data      = training, 
    min.node.size = nodesize,
    mtry      = splitvariables,
    num.trees = ntree)


testing.pred <- predict(rf_bands, testing)

cm<- confusionMatrix(data = testing.pred$predictions, reference = testing$Baumart)

cm.df <- as.data.frame.matrix(cm$table)

cm.acc.df <- as.data.frame(t(cm$overall[1:4]))

kable(cm.acc.df) %>% kable_styling(full_width = F)
```

#### Model using Sentinel-2 indices

```{r rf_index}
# predict on sentinel-2 index bands only

predictors <- names(training %>% select(- contains("band."), -Baumart))

f <- as.formula(paste("Baumart", paste(predictors, collapse=" + "), sep=" ~ "))

# ranger RF model
rf_indices <- ranger(
    formula   = f, 
    data      = training, 
    min.node.size = nodesize,
    mtry      = splitvariables,
    num.trees = ntree)


testing.pred <- predict(rf_indices, testing)

cm<- confusionMatrix(data = testing.pred$predictions, reference = testing$Baumart)

cm.df <- as.data.frame.matrix(cm$table)

cm.acc.df <- as.data.frame(t(cm$overall[1:4]))

kable(cm.acc.df) %>% kable_styling(full_width = F)
```

#### Model using Sentinel-2 bands and indices

```{r rf_all}
# predict on sentinel-2 bands and index bands

predictors <- names(training %>% select(-Baumart))

f <- as.formula(paste("Baumart", paste(predictors, collapse=" + "), sep=" ~ "))

# ranger RF model
rf_all <- ranger(
    formula   = f, 
    data      = training, 
    min.node.size = nodesize,
    mtry      = splitvariables,
    num.trees = ntree)


testing.pred <- predict(rf_all, testing)

cm<- confusionMatrix(data = testing.pred$predictions, reference = testing$Baumart)

cm.df <- as.data.frame.matrix(cm$table)

cm.acc.df <- as.data.frame(t(cm$overall[1:4]))

kable(cm.acc.df) %>% kable_styling(full_width = F)
```

### Conclusion

> Neither the Hyperparameter (min.node.size, mtry) nor the prediction variables had significant impact for tuning the model above certain level!
